{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Univariate LSTM Models\n",
    "\n",
    "We will demonstrate a number of variations of the LSTM model for univariate time series forecasting. This section is divided into six parts; they are:\n",
    "1. Data Preparation\n",
    "2. Vanilla LSTM\n",
    "3. Stacked LSTM\n",
    "4. Bidirectional LSTM 5. CNN-LSTM\n",
    "6. ConvLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.2 Vanilla LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Key to LSTMs is that they offer native support for sequences. Unlike a CNN that reads across the entire input vector, the LSTM model reads one time step of the sequence at a time and builds up an internal state representation that can be used as a learned context for making a prediction. \n",
    "- the model will expect the input component of training data to have the dimensions or shape: [samples, timesteps, features]. \n",
    "- Running the example splits the univariate series into six samples where each sample has three input time steps and one output time step: <br>\n",
    "[10 20 30] 40 <br>\n",
    "[20 30 40] 50 <br>\n",
    "[30 40 50] 60 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jianhua/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0613 11:14:42.263786 4596716992 deprecation_wrapper.py:118] From /Users/Jianhua/anaconda/lib/python3.6/site-packages/tensorflow/__init__.py:99: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
      "\n",
      "W0613 11:14:42.264562 4596716992 deprecation_wrapper.py:118] From /Users/Jianhua/anaconda/lib/python3.6/site-packages/tensorflow/__init__.py:99: The name tf.AttrValue is deprecated. Please use tf.compat.v1.AttrValue instead.\n",
      "\n",
      "W0613 11:14:42.265146 4596716992 deprecation_wrapper.py:118] From /Users/Jianhua/anaconda/lib/python3.6/site-packages/tensorflow/__init__.py:99: The name tf.COMPILER_VERSION is deprecated. Please use tf.version.COMPILER_VERSION instead.\n",
      "\n",
      "W0613 11:14:42.265807 4596716992 deprecation_wrapper.py:118] From /Users/Jianhua/anaconda/lib/python3.6/site-packages/tensorflow/__init__.py:99: The name tf.CXX11_ABI_FLAG is deprecated. Please use tf.sysconfig.CXX11_ABI_FLAG instead.\n",
      "\n",
      "W0613 11:14:42.266464 4596716992 deprecation_wrapper.py:118] From /Users/Jianhua/anaconda/lib/python3.6/site-packages/tensorflow/__init__.py:99: The name tf.ConditionalAccumulator is deprecated. Please use tf.compat.v1.ConditionalAccumulator instead.\n",
      "\n",
      "W0613 11:14:42.764364 4596716992 deprecation_wrapper.py:118] From /Users/Jianhua/anaconda/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0613 11:14:43.216324 4596716992 deprecation.py:323] From /Users/Jianhua/anaconda/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1251: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[102.76117]]\n"
     ]
    }
   ],
   "source": [
    "# univariate lstm example \n",
    "from numpy import array \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM \n",
    "from keras.layers import Dense \n",
    "\n",
    "# split a univariate sequence into samples \n",
    "def split_sequence(sequence, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern \n",
    "        end_idx = i + n_steps\n",
    "        # check if we are beyond the sequence\n",
    "        if end_idx > len(sequence) - 1:\n",
    "            break \n",
    "        # gather input and output parts of the pattern \n",
    "        seq_x, seq_y = sequence[i:end_idx], sequence[end_idx]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)\n",
    "\n",
    "# define input sequence\n",
    "raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "# choose a number of time steps\n",
    "n_steps = 3\n",
    "# split into samples\n",
    "X, y = split_sequence(raw_seq, n_steps)\n",
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "n_features = 1\n",
    "X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "\n",
    "# define model \n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model \n",
    "model.fit(X, y, epochs=200, verbose=0)\n",
    "# demonstrate prediction\n",
    "x_input = array([70, 80, 90])\n",
    "x_input = x_input.reshape((1, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.3 Stacked LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Multiple hidden LSTM layers can be stacked one on top of another in what is referred to as a Stacked LSTM model. \n",
    "- A LSTM layer requires a three-dimensional input and LSTMs by default will produce a two-dimensional output as an interpretation from the end of the sequence.\n",
    "- setting the return sequences=True argument on the layer allows us to have 3D output from hidden LSTM layer as input to the next. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[103.09099]]\n"
     ]
    }
   ],
   "source": [
    "# univariate lstm example \n",
    "from numpy import array \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM \n",
    "from keras.layers import Dense \n",
    "\n",
    "# split a univariate sequence into samples \n",
    "def split_sequence(sequence, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern \n",
    "        end_idx = i + n_steps\n",
    "        # check if we are beyond the sequence\n",
    "        if end_idx > len(sequence) - 1:\n",
    "            break \n",
    "        # gather input and output parts of the pattern \n",
    "        seq_x, seq_y = sequence[i:end_idx], sequence[end_idx]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)\n",
    "\n",
    "# define input sequence\n",
    "raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "# choose a number of time steps\n",
    "n_steps = 3\n",
    "# split into samples\n",
    "X, y = split_sequence(raw_seq, n_steps)\n",
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "n_features = 1\n",
    "X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "\n",
    "# define model \n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\n",
    "model.add(LSTM(50, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model \n",
    "model.fit(X, y, epochs=200, verbose=0)\n",
    "# demonstrate prediction\n",
    "x_input = array([70, 80, 90])\n",
    "x_input = x_input.reshape((1, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.4 Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On some sequence prediction problems, it can be beneficial to allow the LSTM model to learn the input sequence both forward and backwards and concatenate both interpretations. This is called a Bidirectional LSTM.\n",
    "- We can implement a Bidirectional LSTM for univariate time series forecasting by wrapping the first hidden layer in a wrapper layer called Bidirectional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101.49148]]\n"
     ]
    }
   ],
   "source": [
    "# univariate lstm example \n",
    "from numpy import array \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM \n",
    "from keras.layers import Dense \n",
    "from keras.layers import Bidirectional \n",
    "\n",
    "# split a univariate sequence into samples \n",
    "def split_sequence(sequence, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern \n",
    "        end_idx = i + n_steps\n",
    "        # check if we are beyond the sequence\n",
    "        if end_idx > len(sequence) - 1:\n",
    "            break \n",
    "        # gather input and output parts of the pattern \n",
    "        seq_x, seq_y = sequence[i:end_idx], sequence[end_idx]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)\n",
    "\n",
    "# define input sequence\n",
    "raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "# choose a number of time steps\n",
    "n_steps = 3\n",
    "# split into samples\n",
    "X, y = split_sequence(raw_seq, n_steps)\n",
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "n_features = 1\n",
    "X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "\n",
    "# define model \n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(50, activation='relu', input_shape=(n_steps, n_features))))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model \n",
    "model.fit(X, y, epochs=200, verbose=0)\n",
    "# demonstrate prediction\n",
    "x_input = array([70, 80, 90])\n",
    "x_input = x_input.reshape((1, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.5 CNN-LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The CNN can be very effective at automatically extracting and learning features from one-dimensional sequence data such as univariate time series data. \n",
    "- A CNN model can be used in a hybrid model with an LSTM backend where the CNN is used to interpret subsequences of input that together are provided as a sequence to an LSTM model to interpret. This hybrid model is called a CNN-LSTM.\n",
    "- The first step is to split the input sequences into subsequences that can be processed by the CNN model. The CNN can interpret each subsequence of two time steps and provide a time series of interpretations of the subsequences to the LSTM model to process as input. \n",
    "- We can parameterize this and define the number of subsequences as n seq and the number of time steps per subsequence as n steps. The input data can then be reshaped to have the required structure: [samples, subsequences, timesteps, features].\n",
    "- We want to reuse the same CNN model when reading in each sub-sequence of data separately. This can be achieved by wrapping the entire CNN model in a TimeDistributed wrapper that will apply the entire model once per input, in this case, once per input subsequence. \n",
    " - The CNN model first has a convolutional layer for reading across the subsequence that requires a number of filters and a kernel size to be specified. \n",
    " - The number of filters is the number of reads or interpretations of the input sequence. \n",
    " - The kernel size is the number of time steps included of each read operation of the input sequence. \n",
    " - The convolution layer is followed by a max pooling layer that distills the filter maps down to 1/4 of their size that includes the most salient features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[102.983604]]\n"
     ]
    }
   ],
   "source": [
    "# univariate cnn lstm example \n",
    "from numpy import array \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense \n",
    "from keras.layers import Flatten \n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "# split a univariate sequence into samples \n",
    "def split_sequence(sequence, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern \n",
    "        end_idx = i + n_steps\n",
    "        # check if we are beyond the sequence\n",
    "        if end_idx > len(sequence) - 1:\n",
    "            break \n",
    "        # gather input and output parts of the pattern \n",
    "        seq_x, seq_y = sequence[i:end_idx], sequence[end_idx]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)\n",
    "\n",
    "# define input sequence\n",
    "raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "# choose a number of time steps\n",
    "n_steps = 4\n",
    "# split into samples\n",
    "X, y = split_sequence(raw_seq, n_steps)\n",
    "# reshape from [samples, timesteps] into [samples, subsequences, timesteps, features]\n",
    "n_features = 1\n",
    "n_seq = 2\n",
    "n_steps = 2\n",
    "X = X.reshape((X.shape[0], n_seq, n_steps, n_features))\n",
    "\n",
    "# define model \n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(Conv1D(filters=64, kernel_size=1, activation='relu'), input_shape=(None, n_steps, n_features)))\n",
    "model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(LSTM(50, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model \n",
    "model.fit(X, y, epochs=500, verbose=0)\n",
    "# demonstrate prediction\n",
    "x_input = array([60, 70, 80, 90])\n",
    "x_input = x_input.reshape((1, n_seq, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.6 ConvLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A type of LSTM related to the CNN-LSTM is the ConvLSTM, where the convolutional reading of input is built directly into each LSTM unit. The ConvLSTM was developed for reading two-dimensional spatial-temporal data, but can be adapted for use with univariate time series forecasting. The layer expects input as a sequence of two-dimensional images, therefore the shape of input data must be: [samples, timesteps, rows, columns, features].\n",
    "- we can split each sample into subsequences where timesteps will become the number of subsequences, or n seq, and columns will be the number of time steps for each subsequence, or n steps. The number of rows is fixed at 1 as we are working with one-dimensional data. \n",
    "- CNN+LSTM vs ConvLSTM(https://www.quora.com/What-is-the-difference-between-ConvLSTM-and-CNN-LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[103.91734]]\n"
     ]
    }
   ],
   "source": [
    "# univariate cnn lstm example \n",
    "from numpy import array \n",
    "from keras.models import Sequential\n",
    "from keras.layers import ConvLSTM2D\n",
    "from keras.layers import Dense \n",
    "from keras.layers import Flatten \n",
    "\n",
    "# split a univariate sequence into samples \n",
    "def split_sequence(sequence, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern \n",
    "        end_idx = i + n_steps\n",
    "        # check if we are beyond the sequence\n",
    "        if end_idx > len(sequence) - 1:\n",
    "            break \n",
    "        # gather input and output parts of the pattern \n",
    "        seq_x, seq_y = sequence[i:end_idx], sequence[end_idx]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)\n",
    "\n",
    "# define input sequence\n",
    "raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "# choose a number of time steps\n",
    "n_steps = 4\n",
    "# split into samples\n",
    "X, y = split_sequence(raw_seq, n_steps)\n",
    "# reshape from [samples, timesteps] into [samples, subsequences, timesteps, features]\n",
    "n_features = 1\n",
    "n_seq = 2\n",
    "n_steps = 2\n",
    "X = X.reshape((X.shape[0], n_seq, 1, n_steps, n_features))\n",
    "\n",
    "# define model \n",
    "model = Sequential()\n",
    "model.add(ConvLSTM2D(filters=64, kernel_size=(1,2), activation='relu', input_shape=(n_seq, 1, n_steps, n_features)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model \n",
    "model.fit(X, y, epochs=500, verbose=0)\n",
    "# demonstrate prediction\n",
    "x_input = array([60, 70, 80, 90])\n",
    "x_input = x_input.reshape((1, n_seq, 1, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Multivariate LSTM Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main models that we may require with multivariate time series data; they are:\n",
    "1. Multiple Input Series. \n",
    "2. Multiple Parallel Series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.1 Multiple Input Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[205.02939]]\n"
     ]
    }
   ],
   "source": [
    "# multivariate lstm example\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "\n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequences)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps\n",
    "\t\t# check if we are beyond the dataset\n",
    "\t\tif end_ix > len(sequences):\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn array(X), array(y)\n",
    "\n",
    "# define input sequence\n",
    "in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
    "in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\n",
    "out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n",
    "# convert to [rows, columns] structure\n",
    "in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
    "in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
    "out_seq = out_seq.reshape((len(out_seq), 1))\n",
    "# horizontally stack columns\n",
    "dataset = hstack((in_seq1, in_seq2, out_seq))\n",
    "# choose a number of time steps\n",
    "n_steps = 3\n",
    "# convert into input/output\n",
    "X, y = split_sequences(dataset, n_steps)\n",
    "# the dataset knows the number of features, e.g. 2\n",
    "n_features = X.shape[2]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model\n",
    "model.fit(X, y, epochs=200, verbose=0)\n",
    "# demonstrate prediction\n",
    "x_input = array([[80, 85], [90, 95], [100, 105]])\n",
    "x_input = x_input.reshape((1, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.2 Multiple Parallel Series\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An alternate time series problem is the case where there are multiple parallel time series and a value must be predicted for each. We may want to predict the value for each of the three time series for the next time step. This might be referred to as multivariate forecasting. Again, the data must be split into input/output samples in order to train a model. The first sample of this dataset would be: <br>\n",
    "Input: <br>\n",
    "10, 15, 25 <br>\n",
    "20, 25, 45 <br>\n",
    "30, 35, 65 <br>\n",
    "output: <br>\n",
    "40, 45, 85 <br>\n",
    "\n",
    "-  Any of the varieties of LSTMs in the previous section can be used, such as a Vanilla, Stacked, Bidirectional, CNN, or ConvLSTM model. \n",
    "- We will use a Stacked LSTM where the number of time steps and parallel series (features) are specified for the input layer via the input shape argument. The number of parallel series is also used in the specification of the number of values to predict by the model in the output layer; again, this is three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101.46164  107.532585 207.83504 ]]\n"
     ]
    }
   ],
   "source": [
    "# multivariate output stacked lstm example\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(sequences)-1:\n",
    "            break\n",
    "            # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix, :]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)\n",
    "\n",
    "# define input sequence\n",
    "in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
    "in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\n",
    "out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n",
    "# convert to [rows, columns] structure\n",
    "in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
    "in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
    "out_seq = out_seq.reshape((len(out_seq), 1))\n",
    "# horizontally stack columns\n",
    "dataset = hstack((in_seq1, in_seq2, out_seq))\n",
    "# choose a number of time steps\n",
    "n_steps = 3\n",
    "# convert into input/output\n",
    "X, y = split_sequences(dataset, n_steps)\n",
    "# the dataset knows the number of features, e.g. 2\n",
    "n_features = X.shape[2]\n",
    "\n",
    "# define model \n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\n",
    "model.add(LSTM(100, activation='relu'))\n",
    "model.add(Dense(n_features))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model \n",
    "model.fit(X, y, epochs=400, verbose=0)\n",
    "# demonstrate prediction\n",
    "x_input = array([[70, 75, 145], [80, 85, 165], [90, 95, 185]])\n",
    "x_input = x_input.reshape((1, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4 Multi-step LSTM Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A time series forecasting problem that requires a prediction of multiple time steps into the future can be referred to as multi-step time series forecasting. Specifically, these are problems where the forecast horizon or interval is more than one time step. There are two main types of LSTM models that can be used for multi-step forecasting; they are:<br>\n",
    "1. Vector Output Model\n",
    "2. Encoder-Decoder Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4.2 Vector Output Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Like other types of neural network models, the LSTM can output a vector directly that can be interpreted as a multi-step forecast. This approach was seen in the previous section were one time step of each output time series was forecasted as a vector. \n",
    "- Any of the presented LSTM model types could be used, such as Vanilla, Stacked, Bidirectional, CNN-LSTM, or ConvLSTM. Below defines a Stacked LSTM for multi-step forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[114.540436 130.8946  ]]\n"
     ]
    }
   ],
   "source": [
    "# univariate multi-step vector-output stacked lstm example\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "\n",
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps_in, n_steps_out):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequence)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps_in\n",
    "\t\tout_end_ix = end_ix + n_steps_out\n",
    "\t\t# check if we are beyond the sequence\n",
    "\t\tif out_end_ix > len(sequence):\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn array(X), array(y)\n",
    "\n",
    "# define input sequence\n",
    "raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "# choose a number of time steps\n",
    "n_steps_in, n_steps_out = 3, 2\n",
    "# split into samples\n",
    "X, y = split_sequence(raw_seq, n_steps_in, n_steps_out)\n",
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "n_features = 1\n",
    "X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps_in, n_features)))\n",
    "model.add(LSTM(100, activation='relu'))\n",
    "model.add(Dense(n_steps_out))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model\n",
    "model.fit(X, y, epochs=50, verbose=0)\n",
    "# demonstrate prediction\n",
    "x_input = array([70, 80, 90])\n",
    "x_input = x_input.reshape((1, n_steps_in, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4.3 Encoder-Decoder Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A model specifically developed for forecasting variable length output sequences is called the Encoder-Decoder LSTM. The model was designed for prediction problems where there are both input and output sequences, so-called sequence-to-sequence, or seq2seq problems, such as translating text from one language to another. \n",
    "- The encoder is a model responsible for reading and interpreting the input sequence. The output of the encoder is a fixed length vector that represents the model’s interpretation of the sequence. The encoder is traditionally a Vanilla LSTM model, although other encoder models can be used such as Stacked, Bidirectional, and CNN models.\n",
    "- The decoder uses the output of the encoder as an input. First, the fixed-length output of the encoder is repeated, once for each required time step in the output sequence.\n",
    "- This sequence is then provided to an LSTM decoder model. The model must output a value for each value in the output time step, which can be interpreted by a single output model.\n",
    "- We can use the same output layer or layers to make each one-step prediction in the output sequence. This can be achieved by wrapping the output part of the model in a TimeDistributed wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[106.76402]\n",
      "  [117.97816]]]\n"
     ]
    }
   ],
   "source": [
    "# univariate multi-step encoder-decoder lstm example\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "\n",
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps_in, n_steps_out):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequence)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps_in\n",
    "\t\tout_end_ix = end_ix + n_steps_out\n",
    "\t\t# check if we are beyond the sequence\n",
    "\t\tif out_end_ix > len(sequence):\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn array(X), array(y)\n",
    "\n",
    "# define input sequence\n",
    "raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "# choose a number of time steps\n",
    "n_steps_in, n_steps_out = 3, 2\n",
    "# split into samples\n",
    "X, y = split_sequence(raw_seq, n_steps_in, n_steps_out)\n",
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "n_features = 1\n",
    "X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "y = y.reshape((y.shape[0], y.shape[1], n_features))\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', input_shape=(n_steps_in, n_features)))\n",
    "model.add(RepeatVector(n_steps_out))\n",
    "model.add(LSTM(100, activation='relu', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model\n",
    "model.fit(X, y, epochs=100, verbose=0)\n",
    "# demonstrate prediction\n",
    "x_input = array([70, 80, 90])\n",
    "x_input = x_input.reshape((1, n_steps_in, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.5 Multivariate Multi-step LSTM Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Multiple Input Multi-step Output.\n",
    "2. Multiple Parallel Input and Multi-step Output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.5.1 Multiple Input Multi-step Output\n",
    "\n",
    "There are those multivariate time series forecasting problems where the output series is separate but dependent upon the input time series, and multiple time steps are required for the output series. <br>\n",
    "input: <br>\n",
    "10, 15 <br>\n",
    "20, 25 <br>\n",
    "30, 35 <br>\n",
    "output: <br>\n",
    "65 <br>\n",
    "85 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[189.73099 213.11856]]\n"
     ]
    }
   ],
   "source": [
    "# multivariate multi-step stacked lstm example\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "\n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequences)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps_in\n",
    "\t\tout_end_ix = end_ix + n_steps_out-1\n",
    "\t\t# check if we are beyond the dataset\n",
    "\t\tif out_end_ix > len(sequences):\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn array(X), array(y)\n",
    "\n",
    "# define input sequence\n",
    "in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
    "in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\n",
    "out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n",
    "# convert to [rows, columns] structure\n",
    "in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
    "in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
    "out_seq = out_seq.reshape((len(out_seq), 1))\n",
    "# horizontally stack columns\n",
    "dataset = hstack((in_seq1, in_seq2, out_seq))\n",
    "# choose a number of time steps\n",
    "n_steps_in, n_steps_out = 3, 2\n",
    "# covert into input/output\n",
    "X, y = split_sequences(dataset, n_steps_in, n_steps_out)\n",
    "# the dataset knows the number of features, e.g. 2\n",
    "n_features = X.shape[2]\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps_in, n_features)))\n",
    "model.add(LSTM(100, activation='relu'))\n",
    "model.add(Dense(n_steps_out))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model\n",
    "model.fit(X, y, epochs=200, verbose=0)\n",
    "# demonstrate prediction\n",
    "x_input = array([[70, 75], [80, 85], [90, 95]])\n",
    "x_input = x_input.reshape((1, n_steps_in, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.5.2 Multiple Parallel Input and Multi-step Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A problem with parallel time series may require the prediction of multiple time steps of each time series. \n",
    "- We may use the last three time steps from each of the three time series as input to the model and predict the next time steps of each of the three time series as output.\n",
    "Input: <br>\n",
    "10, 15, 25 <br>\n",
    "20, 25, 45 <br>\n",
    "30, 35, 65 <br>\n",
    "output: <br>\n",
    "40, 45, 85 <br>\n",
    "50, 55, 105 <br>\n",
    "- We can use either the Vector Output or Encoder-Decoder LSTM to model this problem. In this case, we will use the Encoder-Decoder model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 90.96196  96.90446 186.5906 ]\n",
      "  [101.33786 106.99348 208.46887]]]\n"
     ]
    }
   ],
   "source": [
    "# multivariate multi-step encoder-decoder lstm example\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "\n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequences)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps_in\n",
    "\t\tout_end_ix = end_ix + n_steps_out\n",
    "\t\t# check if we are beyond the dataset\n",
    "\t\tif out_end_ix > len(sequences):\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix:out_end_ix, :]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn array(X), array(y)\n",
    "\n",
    "# define input sequence\n",
    "in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
    "in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\n",
    "out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n",
    "# convert to [rows, columns] structure\n",
    "in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
    "in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
    "out_seq = out_seq.reshape((len(out_seq), 1))\n",
    "# horizontally stack columns\n",
    "dataset = hstack((in_seq1, in_seq2, out_seq))\n",
    "# choose a number of time steps\n",
    "n_steps_in, n_steps_out = 3, 2\n",
    "# covert into input/output\n",
    "X, y = split_sequences(dataset, n_steps_in, n_steps_out)\n",
    "# the dataset knows the number of features, e.g. 2\n",
    "n_features = X.shape[2]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(200, activation='relu', input_shape=(n_steps_in, n_features)))\n",
    "model.add(RepeatVector(n_steps_out))\n",
    "model.add(LSTM(200, activation='relu', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(n_features)))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model\n",
    "model.fit(X, y, epochs=300, verbose=0)\n",
    "# demonstrate prediction\n",
    "x_input = array([[60, 65, 125], [70, 75, 145], [80, 85, 165]])\n",
    "x_input = x_input.reshape((1, n_steps_in, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example fits the model and predicts the values for each of the three time steps for the next two time steps beyond the end of the dataset. We would expect the values for these series and time steps to be as follows: <br>\n",
    "90, 95, 185 <br>\n",
    "100, 105, 205 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.7 Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.7.1 Books\n",
    "- Deep Learning, 2016. https://amzn.to/2MQyLVZ\n",
    "- Deep Learning with Python, 2017. https://amzn.to/2vMRiMe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.7.2 Papers\n",
    "- Long Short-Term Memory, 1997. https://ieeexplore.ieee.org/document/6795963/.\n",
    "- Learning to Forget: Continual Prediction with LSTM, 1999. https://ieeexplore.ieee.org/document/818041/\n",
    "- Recurrent Nets that Time and Count, 2000. https://ieeexplore.ieee.org/document/861302/\n",
    "- LSTM: A Search Space Odyssey, 2017. https://arxiv.org/abs/1503.04069\n",
    "- Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting, 2015. https://arxiv.org/abs/1506.04214v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.8 Summary\n",
    "In this tutorial, you discovered how to develop a suite of LSTM models for a range of standard time series forecasting problems. Specifically, you learned:\n",
    "- How to develop LSTM models for univariate time series forecasting.\n",
    "- How to develop LSTM models for multivariate time series forecasting. 􏰀\n",
    "- How to develop LSTM models for multi-step time series forecasting.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
