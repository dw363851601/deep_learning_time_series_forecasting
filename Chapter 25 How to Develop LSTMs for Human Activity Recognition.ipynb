{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recently, deep learning methods such as recurrent neural networks like as LSTMs and variations that make use of one-dimensional convolutional neural networks or CNNs have been shown to provide state-of-the-art results on challenging activity recognition tasks with little or no data feature engineering, instead using feature learning on raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25.1 Tutorial Overview\n",
    "This tutorial is divided into four parts; they are:\n",
    "1. Activity Recognition Using Smartphones Dataset \n",
    "2. LSTM Model\n",
    "3. CNN-LSTM Model\n",
    "4. ConvLSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25.2 Activity Recognition Using Smartphones Dataset\n",
    "\n",
    "Human Activity Recognition, or HAR for short, is the problem of predicting what a person is doing based on a trace of their movement using sensors. A standard human activity recognition dataset is the Activity Recognition Using Smartphones made available in 2012. For more information on this dataset, see Chapter 22."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25.3 LSTM Model\n",
    "\n",
    "LSTM network models are a type of recurrent neural network that are able to learn and remember over long sequences of input data. They are intended for use with data that is comprised of long sequences of data, up to 200 to 400 time steps. They may be a good fit for this problem. The model can support multiple parallel sequences of input data, such as each axis of the accelerometer and gyroscope data. The model learns to extract features from sequences of observations and how to map the internal features to different activity types.\n",
    "\n",
    "This section is divided into four parts; they are:\n",
    "1. Load Data\n",
    "2. Fit and Evaluate Model \n",
    "3. Summarize Results\n",
    "4. Complete Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25.3.2 Fit and Evaluate Model\n",
    "\n",
    "We can define a function named evaluate model() that takes the train and test dataset, fits a model on the training dataset, evaluates it on the test dataset, and returns an estimate of the model’s performance. First, we must define the LSTM model using the Keras deep learning library. The model requires a three-dimensional input with [samples, timesteps, features].\n",
    "\n",
    "This is exactly how we have loaded the data, where one sample is one window of the time series data, each window has 128 time steps, and a time step has nine variables or features.\n",
    "\n",
    "We will define the model as having a single LSTM hidden layer. This is followed by a dropout layer intended to reduce overfitting of the model to the training data. Finally, a dense fully connected layer is used to interpret the features extracted by the LSTM hidden layer, before a final output layer is used to make predictions. \n",
    "\n",
    "Note, it is common to not shuffle sequence data when fitting an LSTM. Here we do shuffle the windows of input data during training (the default). In this problem, we are interested in harnessing the LSTMs ability to learn and extract features across the time steps in a window, not across windows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25.3.3 Summarize Results\n",
    "\n",
    "We cannot judge the skill of the model from a single evaluation. The reason for this is that neural networks are stochastic, meaning that a different specific model will result when training the same model configuration on the same data. This is a feature of the network in that it gives the model its adaptive ability, but requires a slightly more complicated evaluation of the model. We will repeat the evaluation of the model multiple times, then summarize the performance of the model across each of those runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0703 11:01:48.495784 4478961088 deprecation_wrapper.py:119] From /Users/Jianhua/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0703 11:01:48.517338 4478961088 deprecation_wrapper.py:119] From /Users/Jianhua/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0703 11:01:48.519974 4478961088 deprecation_wrapper.py:119] From /Users/Jianhua/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0703 11:01:48.779546 4478961088 deprecation_wrapper.py:119] From /Users/Jianhua/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0703 11:01:48.792837 4478961088 deprecation.py:506] From /Users/Jianhua/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0703 11:01:48.843791 4478961088 deprecation_wrapper.py:119] From /Users/Jianhua/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0703 11:01:48.873154 4478961088 deprecation_wrapper.py:119] From /Users/Jianhua/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0703 11:01:49.018087 4478961088 deprecation.py:323] From /Users/Jianhua/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">#1: 86.597\n",
      ">#2: 89.956\n",
      ">#3: 88.225\n",
      ">#4: 88.734\n",
      ">#5: 87.750\n",
      ">#6: 91.347\n",
      ">#7: 91.279\n",
      ">#8: 89.379\n",
      ">#9: 90.770\n",
      ">#10: 87.513\n",
      "[86.59653885307091, 89.95588734306074, 88.22531387852052, 88.73430607397353, 87.75025449609772, 91.34713267729894, 91.27926705123855, 89.37902952154734, 90.77027485578554, 87.51272480488632]\n",
      "Accuracy: 89.155% (+/-1.577)\n"
     ]
    }
   ],
   "source": [
    "# lstm model for the har dataset\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# load a single file as a numpy array\n",
    "def load_file(filepath):\n",
    "\tdataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
    "\treturn dataframe.values\n",
    "\n",
    "# load a list of files and return as a 3d numpy array\n",
    "def load_group(filenames, prefix=''):\n",
    "\tloaded = list()\n",
    "\tfor name in filenames:\n",
    "\t\tdata = load_file(prefix + name)\n",
    "\t\tloaded.append(data)\n",
    "\t# stack group so that features are the 3rd dimension\n",
    "\tloaded = dstack(loaded)\n",
    "\treturn loaded\n",
    "\n",
    "# load a dataset group, such as train or test\n",
    "def load_dataset_group(group, prefix=''):\n",
    "\tfilepath = prefix + group + '/Inertial Signals/'\n",
    "\t# load all 9 files as a single array\n",
    "\tfilenames = list()\n",
    "\t# total acceleration\n",
    "\tfilenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
    "\t# body acceleration\n",
    "\tfilenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
    "\t# body gyroscope\n",
    "\tfilenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
    "\t# load input data\n",
    "\tX = load_group(filenames, filepath)\n",
    "\t# load class output\n",
    "\ty = load_file(prefix + group + '/y_'+group+'.txt')\n",
    "\treturn X, y\n",
    "\n",
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(prefix=''):\n",
    "\t# load all train\n",
    "\ttrainX, trainy = load_dataset_group('train', prefix + 'HARDataset/')\n",
    "\t# load all test\n",
    "\ttestX, testy = load_dataset_group('test', prefix + 'HARDataset/')\n",
    "\t# zero-offset class values\n",
    "\ttrainy = trainy - 1\n",
    "\ttesty = testy - 1\n",
    "\t# one hot encode y\n",
    "\ttrainy = to_categorical(trainy)\n",
    "\ttesty = to_categorical(testy)\n",
    "\treturn trainX, trainy, testX, testy\n",
    "\n",
    "# fit and evaluate a model\n",
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "\tverbose, epochs, batch_size = 0, 15, 64\n",
    "\tn_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(LSTM(100, input_shape=(n_timesteps,n_features)))\n",
    "\tmodel.add(Dropout(0.5))\n",
    "\tmodel.add(Dense(100, activation='relu'))\n",
    "\tmodel.add(Dense(n_outputs, activation='softmax'))\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\t# fit network\n",
    "\tmodel.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\t# evaluate model\n",
    "\t_, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "\treturn accuracy\n",
    "\n",
    "# summarize scores\n",
    "def summarize_results(scores):\n",
    "\tprint(scores)\n",
    "\tm, s = mean(scores), std(scores)\n",
    "\tprint('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n",
    "\n",
    "# run an experiment\n",
    "def run_experiment(repeats=10):\n",
    "\t# load data\n",
    "\ttrainX, trainy, testX, testy = load_dataset()\n",
    "\t# repeat experiment\n",
    "\tscores = list()\n",
    "\tfor r in range(repeats):\n",
    "\t\tscore = evaluate_model(trainX, trainy, testX, testy)\n",
    "\t\tscore = score * 100.0\n",
    "\t\tprint('>#%d: %.3f' % (r+1, score))\n",
    "\t\tscores.append(score)\n",
    "\t# summarize results\n",
    "\tsummarize_results(scores)\n",
    "\n",
    "# run the experiment\n",
    "run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model performed well, achieving a classification accuracy of about 89.15% trained on the raw dataset, with a standard deviation of about 1.577. This is a good result, considering that the original paper published a result of 89%, trained on the dataset with heavy domain-specific feature engineering, not the raw dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25.4 CNN-LSTM Model\n",
    "\n",
    "The CNN-LSTM architecture involves using Convolutional Neural Network (CNN) layers for feature extraction on input data combined with LSTMs to support sequence prediction.\n",
    "\n",
    "The CNN-LSTM model will read subsequences of the main sequence in as blocks, extract features from each block, then allow the LSTM to interpret the features extracted from each block. One approach to implementing this model is to split each window of 128 time steps into subsequences for the CNN model to process. \n",
    "\n",
    "We can then define a CNN model that expects to read in sequences with a length of 32 time steps and nine features. The entire CNN model can be wrapped in a __TimeDistributed layer__ to allow the same CNN model to read in each of the four subsequences in the window. The extracted features are then flattened and provided to the LSTM model to read, extracting its own features before a final mapping to an activity is made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0703 13:39:38.438038 4478961088 deprecation_wrapper.py:119] From /Users/Jianhua/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">#1: 90.465\n",
      ">#2: 88.666\n",
      ">#3: 90.872\n",
      ">#4: 92.026\n",
      ">#5: 90.872\n",
      ">#6: 90.635\n",
      ">#7: 90.058\n",
      ">#8: 90.363\n",
      ">#9: 88.225\n",
      ">#10: 89.243\n",
      "[90.46487953851374, 88.66644044791313, 90.87207329487615, 92.02578893790296, 90.87207329487615, 90.63454360366474, 90.05768578215134, 90.36308109942314, 88.22531387852052, 89.24329826942655]\n",
      "Accuracy: 90.143% (+/-1.081)\n"
     ]
    }
   ],
   "source": [
    "# cnn lstm model for the har dataset\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# load a single file as a numpy array\n",
    "def load_file(filepath):\n",
    "\tdataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
    "\treturn dataframe.values\n",
    "\n",
    "# load a list of files and return as a 3d numpy array\n",
    "def load_group(filenames, prefix=''):\n",
    "\tloaded = list()\n",
    "\tfor name in filenames:\n",
    "\t\tdata = load_file(prefix + name)\n",
    "\t\tloaded.append(data)\n",
    "\t# stack group so that features are the 3rd dimension\n",
    "\tloaded = dstack(loaded)\n",
    "\treturn loaded\n",
    "\n",
    "# load a dataset group, such as train or test\n",
    "def load_dataset_group(group, prefix=''):\n",
    "\tfilepath = prefix + group + '/Inertial Signals/'\n",
    "\t# load all 9 files as a single array\n",
    "\tfilenames = list()\n",
    "\t# total acceleration\n",
    "\tfilenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
    "\t# body acceleration\n",
    "\tfilenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
    "\t# body gyroscope\n",
    "\tfilenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
    "\t# load input data\n",
    "\tX = load_group(filenames, filepath)\n",
    "\t# load class output\n",
    "\ty = load_file(prefix + group + '/y_'+group+'.txt')\n",
    "\treturn X, y\n",
    "\n",
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(prefix=''):\n",
    "\t# load all train\n",
    "\ttrainX, trainy = load_dataset_group('train', prefix + 'HARDataset/')\n",
    "\t# load all test\n",
    "\ttestX, testy = load_dataset_group('test', prefix + 'HARDataset/')\n",
    "\t# zero-offset class values\n",
    "\ttrainy = trainy - 1\n",
    "\ttesty = testy - 1\n",
    "\t# one hot encode y\n",
    "\ttrainy = to_categorical(trainy)\n",
    "\ttesty = to_categorical(testy)\n",
    "\treturn trainX, trainy, testX, testy\n",
    "\n",
    "# fit and evaluate a model\n",
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "\t# define model\n",
    "\tverbose, epochs, batch_size = 0, 25, 64\n",
    "\tn_features, n_outputs = trainX.shape[2], trainy.shape[1]\n",
    "\t# reshape data into time steps of sub-sequences\n",
    "\tn_steps, n_length = 4, 32\n",
    "\ttrainX = trainX.reshape((trainX.shape[0], n_steps, n_length, n_features))\n",
    "\ttestX = testX.reshape((testX.shape[0], n_steps, n_length, n_features))\n",
    "\t# define model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'), input_shape=(None,n_length,n_features)))\n",
    "\tmodel.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu')))\n",
    "\tmodel.add(TimeDistributed(Dropout(0.5)))\n",
    "\tmodel.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "\tmodel.add(TimeDistributed(Flatten()))\n",
    "\tmodel.add(LSTM(100))\n",
    "\tmodel.add(Dropout(0.5))\n",
    "\tmodel.add(Dense(100, activation='relu'))\n",
    "\tmodel.add(Dense(n_outputs, activation='softmax'))\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\t# fit network\n",
    "\tmodel.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\t# evaluate model\n",
    "\t_, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "\treturn accuracy\n",
    "\n",
    "# summarize scores\n",
    "def summarize_results(scores):\n",
    "\tprint(scores)\n",
    "\tm, s = mean(scores), std(scores)\n",
    "\tprint('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n",
    "\n",
    "# run an experiment\n",
    "def run_experiment(repeats=10):\n",
    "\t# load data\n",
    "\ttrainX, trainy, testX, testy = load_dataset()\n",
    "\t# repeat experiment\n",
    "\tscores = list()\n",
    "\tfor r in range(repeats):\n",
    "\t\tscore = evaluate_model(trainX, trainy, testX, testy)\n",
    "\t\tscore = score * 100.0\n",
    "\t\tprint('>#%d: %.3f' % (r+1, score))\n",
    "\t\tscores.append(score)\n",
    "\t# summarize results\n",
    "\tsummarize_results(scores)\n",
    "\n",
    "# run the experiment\n",
    "run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example summarizes the model performance for each of the 10 runs before a final summary of the model’s performance on the test set is reported. We can see that the model achieved a performance of about 90.6% with a standard deviation of about 1%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25.5 ConvLSTM Model\n",
    "\n",
    "A further extension of the CNN-LSTM idea is to perform the convolutions of the CNN (e.g. how the CNN reads the input sequence data) as part of the LSTM. This combination is called a Convolutional LSTM, or ConvLSTM for short, and like the CNN-LSTM is also used for spatiotemporal data. The ConvLSTM2D class, by default, expects input data to have the shape: [samples, time, rows, cols, channels]. Where each time step of data is defined as an image of (rows × columns) data points.\n",
    "\n",
    "In the previous section, we divided a given window of data (128 time steps) into four subsequences of 32 time steps. We can use this same subsequence approach in defining the ConvLSTM2D input where the number of time steps is the number of subsequences in the window, the number of rows is 1 as we are working with one-dimensional data, and the number of columns represents the number of time steps in the subsequence, in this case 32. For this chosen framing of the problem, the input for the ConvLSTM2D would therefore be:\n",
    "- Samples: n, for the number of windows in the dataset.\n",
    "- Time: 4, for the four subsequences that we split a window of 128 time steps into. 􏰀 \n",
    "- Rows: 1, for the one-dimensional shape of each subsequence.\n",
    "- Columns: 32, for the 32 time steps in an input subsequence.\n",
    "- Channels: 9, for the nine input variables.\n",
    "\n",
    "The ConvLSTM2D class requires configuration both in terms of the CNN and the LSTM. This includes specifying the number of filters (e.g. 64), the two-dimensional kernel size, in this case (1 row and 3 columns of the subsequence time steps), and the activation function, in this case rectified linear. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">#1: 91.110\n",
      ">#2: 90.465\n",
      ">#3: 89.108\n",
      ">#4: 90.261\n",
      ">#5: 88.293\n",
      ">#6: 89.820\n",
      ">#7: 89.718\n",
      ">#8: 89.447\n",
      ">#9: 91.381\n",
      ">#10: 90.940\n",
      "[91.10960298608755, 90.46487953851374, 89.10756701730574, 90.26128266033254, 88.29317950458093, 89.82015609093995, 89.71835765387189, 89.44689514760775, 91.38106549032915, 90.93993892093654]\n",
      "Accuracy: 90.054% (+/-0.917)\n"
     ]
    }
   ],
   "source": [
    "# convlstm model for the har dataset\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import ConvLSTM2D\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# load a single file as a numpy array\n",
    "def load_file(filepath):\n",
    "\tdataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
    "\treturn dataframe.values\n",
    "\n",
    "# load a list of files and return as a 3d numpy array\n",
    "def load_group(filenames, prefix=''):\n",
    "\tloaded = list()\n",
    "\tfor name in filenames:\n",
    "\t\tdata = load_file(prefix + name)\n",
    "\t\tloaded.append(data)\n",
    "\t# stack group so that features are the 3rd dimension\n",
    "\tloaded = dstack(loaded)\n",
    "\treturn loaded\n",
    "\n",
    "# load a dataset group, such as train or test\n",
    "def load_dataset_group(group, prefix=''):\n",
    "\tfilepath = prefix + group + '/Inertial Signals/'\n",
    "\t# load all 9 files as a single array\n",
    "\tfilenames = list()\n",
    "\t# total acceleration\n",
    "\tfilenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
    "\t# body acceleration\n",
    "\tfilenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
    "\t# body gyroscope\n",
    "\tfilenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
    "\t# load input data\n",
    "\tX = load_group(filenames, filepath)\n",
    "\t# load class output\n",
    "\ty = load_file(prefix + group + '/y_'+group+'.txt')\n",
    "\treturn X, y\n",
    "\n",
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(prefix=''):\n",
    "\t# load all train\n",
    "\ttrainX, trainy = load_dataset_group('train', prefix + 'HARDataset/')\n",
    "\t# load all test\n",
    "\ttestX, testy = load_dataset_group('test', prefix + 'HARDataset/')\n",
    "\t# zero-offset class values\n",
    "\ttrainy = trainy - 1\n",
    "\ttesty = testy - 1\n",
    "\t# one hot encode y\n",
    "\ttrainy = to_categorical(trainy)\n",
    "\ttesty = to_categorical(testy)\n",
    "\treturn trainX, trainy, testX, testy\n",
    "\n",
    "# fit and evaluate a model\n",
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "\t# define model\n",
    "\tverbose, epochs, batch_size = 0, 25, 64\n",
    "\tn_features, n_outputs = trainX.shape[2], trainy.shape[1]\n",
    "\t# reshape into subsequences (samples, time steps, rows, cols, channels)\n",
    "\tn_steps, n_length = 4, 32\n",
    "\ttrainX = trainX.reshape((trainX.shape[0], n_steps, 1, n_length, n_features))\n",
    "\ttestX = testX.reshape((testX.shape[0], n_steps, 1, n_length, n_features))\n",
    "\t# define model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(ConvLSTM2D(filters=64, kernel_size=(1,3), activation='relu', input_shape=(n_steps, 1, n_length, n_features)))\n",
    "\tmodel.add(Dropout(0.5))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(100, activation='relu'))\n",
    "\tmodel.add(Dense(n_outputs, activation='softmax'))\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\t# fit network\n",
    "\tmodel.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\t# evaluate model\n",
    "\t_, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "\treturn accuracy\n",
    "\n",
    "# summarize scores\n",
    "def summarize_results(scores):\n",
    "\tprint(scores)\n",
    "\tm, s = mean(scores), std(scores)\n",
    "\tprint('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n",
    "\n",
    "# run an experiment\n",
    "def run_experiment(repeats=10):\n",
    "\t# load data\n",
    "\ttrainX, trainy, testX, testy = load_dataset()\n",
    "\t# repeat experiment\n",
    "\tscores = list()\n",
    "\tfor r in range(repeats):\n",
    "\t\tscore = evaluate_model(trainX, trainy, testX, testy)\n",
    "\t\tscore = score * 100.0\n",
    "\t\tprint('>#%d: %.3f' % (r+1, score))\n",
    "\t\tscores.append(score)\n",
    "\t# summarize results\n",
    "\tsummarize_results(scores)\n",
    "\n",
    "# run the experiment\n",
    "run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the prior experiments, running the model prints the performance of the model each time it is fit and evaluated. A summary of the final model performance is presented at the end of the run. We can see that the model does consistently perform well on the problem achieving an accuracy of about 90%, perhaps with fewer resources than the larger CNN-LSTM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25.7 Further Reading\n",
    "This section provides more resources on the topic if you are looking to go deeper.\n",
    "- Getting started with the Keras Sequential model. https://keras.io/getting-started/sequential-model-guide/\n",
    "- Getting started with the Keras functional API. https://keras.io/getting-started/functional-api-guide/\n",
    "- Keras Sequential Model API. https://keras.io/models/sequential/\n",
    "- Keras Core Layers API. https://keras.io/layers/core/\n",
    "- Keras Convolutional Layers API. https://keras.io/layers/convolutional/\n",
    "- Keras Pooling Layers API. https://keras.io/layers/pooling/\n",
    "- Keras Recurrent Layers API. https://keras.io/layers/recurrent/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25.8 Summary\n",
    "In this tutorial, you discovered three recurrent neural network architectures for modeling an activity recognition time series classification problem. Specifically, you learned:\n",
    "- How to develop a Long Short-Term Memory Recurrent Neural Network for human activity recognition.\n",
    "- How to develop a one-dimensional Convolutional Neural Network LSTM, or CNN-LSTM,\n",
    "model.\n",
    "- How to develop a one-dimensional Convolutional LSTM, or ConvLSTM, model for the same problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
