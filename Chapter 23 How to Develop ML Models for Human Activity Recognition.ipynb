{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23.1 Tutorial Overview\n",
    "This tutorial is divided into three parts; they are:\n",
    "1. Activity Recognition Using Smartphones Dataset \n",
    "2. Modeling Feature Engineered Data\n",
    "3. Modeling Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23.3 Modeling Feature Engineered Data\n",
    "\n",
    "The results of methods using the feature-engineered version of the dataset provide a baseline for any methods developed for the raw data version. This section is divided into five parts; they are:\n",
    "1. Load Dataset\n",
    "2. Define Models\n",
    "3. Evaluate Models\n",
    "4. Summarize Results \n",
    "5. Complete Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 23.3.2 Define Models\n",
    "\n",
    "We will evaluate the models using default configurations. We are not looking for optimal configurations of these models at this point, just a general idea of how well sophisticated models with default configurations perform on this problem. We will evaluate a diverse set of nonlinear and ensemble machine learning algorithms, specifically:\n",
    "\n",
    "Nonlinear Algorithms:\n",
    "- k-Nearest Neighbors\n",
    "- Classification and Regression Tree 􏰀 \n",
    "- Support Vector Machine\n",
    "- Naive Bayes\n",
    "\n",
    "Ensemble Algorithms:\n",
    "- Bagged Decision Trees\n",
    "- Random Forest\n",
    "- Extra Trees\n",
    "- Gradient Boosting Machine\n",
    "\n",
    "We will define the models and store them in a dictionary that maps the model object to a short name that will help in analyzing the results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 23.3.3 Evaluate Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The next step is to evaluate the defined models in the loaded dataset. This step is divided into the evaluation of a single model and the evaluation of all of the models. We will evaluate a single model by first fitting it on the training dataset, making a prediction on the test dataset, and then evaluating the prediction using a metric. In this case we will use classification accuracy that will capture the performance (or error) of a model given the balance observations across the six activities (or classes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 23.3.4 Summarize Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to summarize the findings. We can sort all of the results by the classification accuracy in descending order because we are interested in maximizing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined 8 models\n",
      ">knn: 90.329\n",
      ">cart: 86.121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jianhua/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">svm: 94.028\n",
      ">bayes: 77.027\n",
      ">bag: 89.752\n",
      ">rf: 93.112\n",
      ">et: 93.892\n",
      ">gbm: 93.756\n",
      "\n",
      "Name=svm, Score=94.028\n",
      "Name=et, Score=93.892\n",
      "Name=gbm, Score=93.756\n",
      "Name=rf, Score=93.112\n",
      "Name=knn, Score=90.329\n",
      "Name=bag, Score=89.752\n",
      "Name=cart, Score=86.121\n",
      "Name=bayes, Score=77.027\n"
     ]
    }
   ],
   "source": [
    "# spot check ml algorithms on engineered-features from the har dataset\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# load a single file as a numpy array\n",
    "def load_file(filepath):\n",
    "\tdataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
    "\treturn dataframe.values\n",
    "\n",
    "# load a dataset group, such as train or test\n",
    "def load_dataset_group(group, prefix=''):\n",
    "\t# load input data\n",
    "\tX = load_file(prefix + group + '/X_'+group+'.txt')\n",
    "\t# load class output\n",
    "\ty = load_file(prefix + group + '/y_'+group+'.txt')\n",
    "\treturn X, y\n",
    "\n",
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(prefix=''):\n",
    "\t# load all train\n",
    "\ttrainX, trainy = load_dataset_group('train', prefix + 'HARDataset/')\n",
    "\t# load all test\n",
    "\ttestX, testy = load_dataset_group('test', prefix + 'HARDataset/')\n",
    "\t# flatten y\n",
    "\ttrainy, testy = trainy[:,0], testy[:,0]\n",
    "\treturn trainX, trainy, testX, testy\n",
    "\n",
    "# create a dict of standard models to evaluate {name:object}\n",
    "def define_models(models=dict()):\n",
    "\t# nonlinear models\n",
    "\tmodels['knn'] = KNeighborsClassifier(n_neighbors=7)\n",
    "\tmodels['cart'] = DecisionTreeClassifier()\n",
    "\tmodels['svm'] = SVC()\n",
    "\tmodels['bayes'] = GaussianNB()\n",
    "\t# ensemble models\n",
    "\tmodels['bag'] = BaggingClassifier(n_estimators=100)\n",
    "\tmodels['rf'] = RandomForestClassifier(n_estimators=100)\n",
    "\tmodels['et'] = ExtraTreesClassifier(n_estimators=100)\n",
    "\tmodels['gbm'] = GradientBoostingClassifier(n_estimators=100)\n",
    "\tprint('Defined %d models' % len(models))\n",
    "\treturn models\n",
    "\n",
    "# evaluate a single model\n",
    "def evaluate_model(trainX, trainy, testX, testy, model):\n",
    "\t# fit the model\n",
    "\tmodel.fit(trainX, trainy)\n",
    "\t# make predictions\n",
    "\tyhat = model.predict(testX)\n",
    "\t# evaluate predictions\n",
    "\taccuracy = accuracy_score(testy, yhat)\n",
    "\treturn accuracy * 100.0\n",
    "\n",
    "# evaluate a dict of models {name:object}, returns {name:score}\n",
    "def evaluate_models(trainX, trainy, testX, testy, models):\n",
    "\tresults = dict()\n",
    "\tfor name, model in models.items():\n",
    "\t\t# evaluate the model\n",
    "\t\tresults[name] = evaluate_model(trainX, trainy, testX, testy, model)\n",
    "\t\t# show process\n",
    "\t\tprint('>%s: %.3f' % (name, results[name]))\n",
    "\treturn results\n",
    "\n",
    "# print and plot the results\n",
    "def summarize_results(results, maximize=True):\n",
    "\t# create a list of (name, mean(scores)) tuples\n",
    "\tmean_scores = [(k,v) for k,v in results.items()]\n",
    "\t# sort tuples by mean score\n",
    "\tmean_scores = sorted(mean_scores, key=lambda x: x[1])\n",
    "\t# reverse for descending order (e.g. for accuracy)\n",
    "\tif maximize:\n",
    "\t\tmean_scores = list(reversed(mean_scores))\n",
    "\tprint()\n",
    "\tfor name, score in mean_scores:\n",
    "\t\tprint('Name=%s, Score=%.3f' % (name, score))\n",
    "\n",
    "# load dataset\n",
    "trainX, trainy, testX, testy = load_dataset()\n",
    "# get model list\n",
    "models = define_models()\n",
    "# evaluate models\n",
    "results = evaluate_models(trainX, trainy, testX, testy, models)\n",
    "# summarize results\n",
    "summarize_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that both the ExtraTrees ensemble method and the Support Vector Machines nonlinear methods achieve a performance of about 94% accuracy on the test set. This is a great result, exceeding the reported 89% by SVM in the original paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results show what is possible given domain expertise in the preparation of the data and the engineering of domain-specific features. As such, these results can be taken as a performance upper-bound of what could be pursued through more advanced methods that may be able to automatically learn features as part of fitting the model, such as deep learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23.4 Modeling Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined 8 models\n",
      ">knn: 61.893\n",
      ">cart: 72.650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jianhua/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">svm: 76.960\n",
      ">bayes: 72.480\n",
      ">bag: 84.187\n",
      ">rf: 84.900\n",
      ">et: 86.461\n",
      ">gbm: 87.615\n",
      "\n",
      "Name=gbm, Score=87.615\n",
      "Name=et, Score=86.461\n",
      "Name=rf, Score=84.900\n",
      "Name=bag, Score=84.187\n",
      "Name=svm, Score=76.960\n",
      "Name=cart, Score=72.650\n",
      "Name=bayes, Score=72.480\n",
      "Name=knn, Score=61.893\n"
     ]
    }
   ],
   "source": [
    "# spot check on raw data from the har dataset\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# load a single file as a numpy array\n",
    "def load_file(filepath):\n",
    "\tdataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
    "\treturn dataframe.values\n",
    "\n",
    "# load a list of files into a 3D array of [samples, timesteps, features]\n",
    "def load_group(filenames, prefix=''):\n",
    "\tloaded = list()\n",
    "\tfor name in filenames:\n",
    "\t\tdata = load_file(prefix + name)\n",
    "\t\tloaded.append(data)\n",
    "\t# stack group so that features are the 3rd dimension\n",
    "\tloaded = dstack(loaded)\n",
    "\treturn loaded\n",
    "\n",
    "# load a dataset group, such as train or test\n",
    "def load_dataset_group(group, prefix=''):\n",
    "\tfilepath = prefix + group + '/Inertial Signals/'\n",
    "\t# load all 9 files as a single array\n",
    "\tfilenames = list()\n",
    "\t# total acceleration\n",
    "\tfilenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
    "\t# body acceleration\n",
    "\tfilenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
    "\t# body gyroscope\n",
    "\tfilenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
    "\t# load input data\n",
    "\tX = load_group(filenames, filepath)\n",
    "\t# load class output\n",
    "\ty = load_file(prefix + group + '/y_'+group+'.txt')\n",
    "\treturn X, y\n",
    "\n",
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(prefix=''):\n",
    "\t# load all train\n",
    "\ttrainX, trainy = load_dataset_group('train', prefix + 'HARDataset/')\n",
    "\t# load all test\n",
    "\ttestX, testy = load_dataset_group('test', prefix + 'HARDataset/')\n",
    "\t# flatten X\n",
    "\ttrainX = trainX.reshape((trainX.shape[0], trainX.shape[1] * trainX.shape[2]))\n",
    "\ttestX = testX.reshape((testX.shape[0], testX.shape[1] * testX.shape[2]))\n",
    "\t# flatten y\n",
    "\ttrainy, testy = trainy[:,0], testy[:,0]\n",
    "\treturn trainX, trainy, testX, testy\n",
    "\n",
    "# create a dict of standard models to evaluate {name:object}\n",
    "def define_models(models=dict()):\n",
    "\t# nonlinear models\n",
    "\tmodels['knn'] = KNeighborsClassifier(n_neighbors=7)\n",
    "\tmodels['cart'] = DecisionTreeClassifier()\n",
    "\tmodels['svm'] = SVC()\n",
    "\tmodels['bayes'] = GaussianNB()\n",
    "\t# ensemble models\n",
    "\tmodels['bag'] = BaggingClassifier(n_estimators=100)\n",
    "\tmodels['rf'] = RandomForestClassifier(n_estimators=100)\n",
    "\tmodels['et'] = ExtraTreesClassifier(n_estimators=100)\n",
    "\tmodels['gbm'] = GradientBoostingClassifier(n_estimators=100)\n",
    "\tprint('Defined %d models' % len(models))\n",
    "\treturn models\n",
    "\n",
    "# evaluate a single model\n",
    "def evaluate_model(trainX, trainy, testX, testy, model):\n",
    "\t# fit the model\n",
    "\tmodel.fit(trainX, trainy)\n",
    "\t# make predictions\n",
    "\tyhat = model.predict(testX)\n",
    "\t# evaluate predictions\n",
    "\taccuracy = accuracy_score(testy, yhat)\n",
    "\treturn accuracy * 100.0\n",
    "\n",
    "# evaluate a dict of models {name:object}, returns {name:score}\n",
    "def evaluate_models(trainX, trainy, testX, testy, models):\n",
    "\tresults = dict()\n",
    "\tfor name, model in models.items():\n",
    "\t\t# evaluate the model\n",
    "\t\tresults[name] = evaluate_model(trainX, trainy, testX, testy, model)\n",
    "\t\t# show process\n",
    "\t\tprint('>%s: %.3f' % (name, results[name]))\n",
    "\treturn results\n",
    "\n",
    "# print and plot the results\n",
    "def summarize_results(results, maximize=True):\n",
    "\t# create a list of (name, mean(scores)) tuples\n",
    "\tmean_scores = [(k,v) for k,v in results.items()]\n",
    "\t# sort tuples by mean score\n",
    "\tmean_scores = sorted(mean_scores, key=lambda x: x[1])\n",
    "\t# reverse for descending order (e.g. for accuracy)\n",
    "\tif maximize:\n",
    "\t\tmean_scores = list(reversed(mean_scores))\n",
    "\tprint()\n",
    "\tfor name, score in mean_scores:\n",
    "\t\tprint('Name=%s, Score=%.3f' % (name, score))\n",
    "\n",
    "# load dataset\n",
    "trainX, trainy, testX, testy = load_dataset()\n",
    "# get model list\n",
    "models = define_models()\n",
    "# evaluate models\n",
    "results = evaluate_models(trainX, trainy, testX, testy, models)\n",
    "# summarize results\n",
    "summarize_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final results suggest that ensembles of decision trees perform the best on the raw data. Gradient Boosting and Extra Trees perform the best with about 87% and 86% accuracy, about seven points below the best performing models on the feature-engineered version of the dataset. We can also see the drop of SVM to about 72% accuracy. The good performance of ensembles of decision trees may suggest the need for feature selection and the ensemble methods ability to select those features that are most relevant to predicting the associated activity.\n",
    "\n",
    "As noted in the previous section, these results provide a lower-bound on accuracy for any more sophisticated methods that may attempt to learn higher order features automatically (e.g. via feature learning in deep learning methods) from the raw data. In summary, the bounds for such methods extend on this dataset from about 87% accuracy with GBM on the raw data to about 94% with Extra Trees and SVM on the highly processed dataset, [87% to 94%]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23.6 Further Reading\n",
    "This section provides more resources on the topic if you are looking to go deeper.\n",
    "- scikit-learn: Machine Learning in Python. http://scikit-learn.org/stable/\n",
    "- sklearn.metrics.accuracy score API. http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\n",
    "- sklearn.neighbors API. http://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors\n",
    "- sklearn.tree API. http://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree\n",
    "- sklearn.svm API. http://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm\n",
    "- sklearn.naive bayes API. http://scikit-learn.org/stable/modules/classes.html#module-sklearn.naive_bayes\n",
    "- sklearn.ensemble API. http://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23.7 Summary\n",
    "In this tutorial, you discovered how to evaluate a diverse suite of machine learning algorithms on the Activity Recognition Using Smartphones dataset. Specifically, you learned:\n",
    "- How to load and evaluate nonlinear and ensemble machine learning algorithms on the feature-engineered version of the activity recognition dataset.\n",
    "- How to load and evaluate machine learning algorithms on the raw signal data for the activity recognition dataset.\n",
    "- How to define reasonable lower and upper bounds on the expected performance of more sophisticated algorithms capable of feature learning, such as deep learning methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
