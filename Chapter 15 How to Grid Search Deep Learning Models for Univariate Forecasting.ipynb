{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid searching is generally not an operation that we can perform with deep learning methods. This is because deep learning methods often require large amounts of data and large models, together resulting in models that take hours, days, or weeks to train. In those cases __where the datasets are smaller__, such as __univariate time series__, it may be possible to use a grid search to tune the hyperparameters of a deep learning model.\n",
    "\n",
    "- How to develop a generic grid searching framework for tuning model hyperparameters.\n",
    "- How to grid search hyperparameters for a Multilayer Perceptron model on the airline passengers univariate time series forecasting problem.\n",
    "- How to adapt the framework to grid search hyperparameters for convolutional and long short-term memory neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.1 Tutorial Overview\n",
    "This tutorial is divided into five parts; they are:\n",
    "1. Time Series Problem\n",
    "2. Grid Search Framework\n",
    "3. Multilayer Perceptron Model\n",
    "4. Convolutional Neural Network Model\n",
    "5. Long Short-Term Memory Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.2 Time Series Problem\n",
    "1. We will use the monthly airline passenger dataset as this context as it includes the complexity of both trend and seasonal elements. The monthly airline passenger dataset summarizes the monthly total number of international passengers in thousands on for an airline from 1949 to 1960. \n",
    "2. The dataset is monthly and has 12 years, or 144 observations. In our testing, we will use the last year, or 12 observations, as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load and plot monthly airline passengers dataset\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline \n",
    "# load\n",
    "series = read_csv('monthly-airline-passengers.csv', header=0, index_col=0)\n",
    "# summarize shape\n",
    "print(series.shape)\n",
    "# plot\n",
    "pyplot.plot(series)\n",
    "pyplot.xticks([])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " From prior experiments, a naive model can achieve a root mean squared error, or RMSE, of 50.70 (remember the units are thousands of passengers) by persisting the value from 12 months ago (relative index -12). It should be noted that a tuned ETS model can achieve an RMSE of 17.09 and a tuned SARIMA can achieve an RMSE of 13.89. These provide a lower bound on the expectations of a well-tuned deep learning model for this problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.3 Develop a Grid Search Framework\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will develop a grid search test harness that can be used to evaluate a range of hyperparameters for different neural network models, such as MLPs, CNNs, and LSTMs. This section is divided into the following parts:\n",
    "1. Train-Test Split\n",
    "2. Series as Supervised Learning 3. Walk-Forward Validation\n",
    "4. Repeat Evaluation\n",
    "5. Summarize Performance\n",
    "6. Worked Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.3.1 Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the first 11 years (132 observations) for training and the last 12 for the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.3.2 Series as Supervised Learning\n",
    "1. A supervised learning framing of a series means that the data needs to be split into multiple examples that the model learns from and generalizes across. Each sample must have both an input component and an output component. The input component will be some number of prior observations, such as three years, or 36 time steps.\n",
    "2. The output component will be the total sales in the next month because we are interested in developing a model to make one-step forecasts. We can implement this using the shift() function on the Pandas DataFrame. It allows us to shift a column down (forward in time) or back (backward in time). <br>\n",
    "(t) <br>\n",
    "1   <br>\n",
    "2   <br>\n",
    "3   <br>\n",
    "4   <br>\n",
    "This column can be shifted and inserted as a column beforehand: <br>\n",
    "(t-1), 　　(t) <br>\n",
    "Nan,　　1  <br>\n",
    "1,　　　2 <br>\n",
    "2,　　　3 <br>\n",
    "3,　　　4 <br>\n",
    "4,　　　NaN <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.3.3 Walk-Forward Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Time series forecasting models can be evaluated on a test set using walk-forward validation. Walk-forward validation is an approach where the model makes a forecast for each observation in the test dataset one at a time. After each forecast is made for a time step in the test dataset, the true observation for the forecast is added to the test dataset and made available to the model. \n",
    "2. Simpler models can be refit with the observation prior to making the subsequent prediction. More complex models, such as neural networks, are not refit given the much greater computational cost.\n",
    "3. RMSE is calculated as the square root of the average of the squared differences between the forecasts and the actual values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.3.4 Repeat Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Neural network models are stochastic. This means that, given the same model configuration and the same training dataset, __a different internal set of weights will result each time the model is trained that will, in turn, have a different performance__. This is a benefit, allowing the model to be adaptive and find high performing configurations to complex problems. It is also a problem when evaluating the performance of a model and in choosing a final model to use to make predictions.\n",
    "2. To address model evaluation, we will evaluate a model configuration multiple times via walk-forward validation and report the error as the average error across each evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.3.5 Grid Search\n",
    "All that is left is a function to drive the search. We can define a grid search() function that takes the dataset, a list of configurations to search, and the number of observations to use as the test set and perform the search. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.3.6 Worked Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > 53.152\n",
      " > 53.152\n",
      " > 53.152\n",
      " > 53.152\n",
      " > 53.152\n",
      " > 53.152\n",
      " > 53.152\n",
      " > 53.152\n",
      " > 53.152\n",
      " > 53.152\n",
      "> Model[1] 53.152\n",
      " > 126.735\n",
      " > 126.735\n",
      " > 126.735\n",
      " > 126.735\n",
      " > 126.735\n",
      " > 126.735\n",
      " > 126.735\n",
      " > 126.735\n",
      " > 126.735\n",
      " > 126.735\n",
      "> Model[6] 126.735\n",
      " > 50.708\n",
      " > 50.708\n",
      " > 50.708\n",
      " > 50.708\n",
      " > 50.708\n",
      " > 50.708\n",
      " > 50.708\n",
      " > 50.708\n",
      " > 50.708\n",
      " > 50.708\n",
      "> Model[12] 50.708\n",
      " > 97.110\n",
      " > 97.110\n",
      " > 97.110\n",
      " > 97.110\n",
      " > 97.110\n",
      " > 97.110\n",
      " > 97.110\n",
      " > 97.110\n",
      " > 97.110\n",
      " > 97.110\n",
      "> Model[24] 97.110\n",
      " > 110.274\n",
      " > 110.274\n",
      " > 110.274\n",
      " > 110.274\n",
      " > 110.274\n",
      " > 110.274\n",
      " > 110.274\n",
      " > 110.274\n",
      " > 110.274\n",
      " > 110.274\n",
      "> Model[36] 110.274\n",
      "done\n",
      "12 50.708316214732804\n",
      "1 53.1515129919491\n",
      "24 97.10990337413241\n",
      "36 110.27352356753639\n",
      "6 126.73495965991387\n"
     ]
    }
   ],
   "source": [
    "# grid search persistence models for monthly airline passengers data \n",
    "from math import sqrt \n",
    "from numpy import mean \n",
    "from pandas import read_csv \n",
    "from sklearn.metrics import mean_squared_error \n",
    "\n",
    "# split a univariate dataset into train/test sets \n",
    "def train_test_split(data, n_test):\n",
    "    return data[:-n_test], data[-n_test:]\n",
    "\n",
    "# root mean squared error or rmse\n",
    "def measure_rmse(actual, predicted):\n",
    "    return sqrt(mean_squared_error(actual, predicted))\n",
    "\n",
    "def model_fit(train, config):\n",
    "\treturn None\n",
    "\n",
    "# forecast with a pre-fit model\n",
    "def model_predict(model, history, offset):\n",
    "\treturn history[-offset]\n",
    "\n",
    "# walk-forward validation for univariate data\n",
    "def walk_forward_validation(data, n_test, cfg):\n",
    "\tpredictions = []\n",
    "\t# split dataset\n",
    "\ttrain, test = train_test_split(data, n_test)\n",
    "\t# fit model\n",
    "\tmodel = model_fit(train, cfg)\n",
    "\t# seed history with training dataset\n",
    "\thistory = [x for x in train]\n",
    "\t# step over each time-step in the test set\n",
    "\tfor i in range(len(test)):\n",
    "\t\t# fit model and make forecast for history\n",
    "\t\tyhat = model_predict(model, history, cfg)\n",
    "\t\t# store forecast in list of predictions\n",
    "\t\tpredictions.append(yhat)\n",
    "\t\t# add actual observation to history for the next loop\n",
    "\t\thistory.append(test[i])\n",
    "\t# estimate prediction error\n",
    "\terror = measure_rmse(test, predictions)\n",
    "\tprint(' > %.3f' % error)\n",
    "\treturn error\n",
    "\n",
    "# score a model, return None on failure\n",
    "def repeat_evaluate(data, config, n_test, n_repeats=10):\n",
    "\t# convert config to a key\n",
    "\tkey = str(config)\n",
    "\t# fit and evaluate the model n times\n",
    "\tscores = [walk_forward_validation(data, n_test, config) for _ in range(n_repeats)]\n",
    "\t# summarize score\n",
    "\tresult = mean(scores)\n",
    "\tprint('> Model[%s] %.3f' % (key, result))\n",
    "\treturn (key, result)\n",
    "\n",
    "# grid search configs \n",
    "def grid_search(data, cfg_list, n_test):\n",
    "    # evaluate configs \n",
    "    scores = [repeat_evaluate(data, cfg, n_test) for cfg in cfg_list]\n",
    "    # sort configs by error, asc\n",
    "    scores.sort(key=lambda tup: tup[1])\n",
    "    return scores \n",
    "\n",
    "# define dataset\n",
    "series = read_csv('monthly-airline-passengers.csv', header=0, index_col=0)\n",
    "data = series.values\n",
    "# data split\n",
    "n_test = 12\n",
    "# model configs\n",
    "cfg_list = [1, 6, 12, 24, 36]\n",
    "# grid search\n",
    "scores = grid_search(data, cfg_list, n_test)\n",
    "print('done')\n",
    "# list top 10 configs\n",
    "for cfg, error in scores[:10]:\n",
    "\tprint(cfg, error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, as we might have expected, that persisting the value from one year ago (relative offset -12) resulted in the best performance for the persistence model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.4 Multilayer Perceptron Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. we will grid search hyperparameters for an MLPs for univariate time series forecasting. We will define a very simple model with one hidden layer and define five hyperparameters to tune. They are:\n",
    "   - n_input: The number of prior inputs to use as input for the model (e.g. 12 months). \n",
    "   - n_nodes: The number of nodes to use in the hidden layer (e.g. 50).\n",
    "   - n_epochs: The number of training epochs (e.g. 1000).\n",
    "   - n_batch: The number of samples to include in each mini-batch (e.g. 32).\n",
    "   - n_diff: The difference order (e.g. 0 or 12).\n",
    "\n",
    "2. Recall that __differencing__ is the transform of the data such that a value of a prior observation is subtracted from the current observation, removing trend or seasonality structure. We will add support for differencing to the grid search test harness, just in case it adds value to your specific problem.\n",
    "\n",
    "3. If the data was differenced, the difference must be inverted for the prediction of the model. This involves adding the value at the relative offset from the history back to the value predicted by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total configs: 8\n",
      " > 19.147\n",
      " > 17.075\n",
      " > 20.223\n",
      " > 19.124\n",
      " > 31.867\n",
      " > 18.374\n",
      " > 18.077\n",
      " > 24.383\n",
      " > 18.735\n",
      " > 42.946\n",
      " > Model[[12, 50, 100, 1, 0]] 22.995\n",
      " > 20.709\n",
      " > 18.532\n",
      " > 22.557\n",
      " > 18.905\n",
      " > 20.716\n",
      " > 20.847\n",
      " > 18.765\n",
      " > 21.490\n",
      " > 18.564\n",
      " > 18.390\n",
      " > Model[[12, 50, 100, 1, 12]] 19.948\n",
      " > 13.966\n",
      " > 70.475\n",
      " > 73.612\n",
      " > 70.590\n",
      " > 73.141\n",
      " > 63.098\n",
      " > 65.062\n",
      " > 49.381\n",
      " > 52.060\n",
      " > 57.599\n",
      " > Model[[12, 50, 100, 150, 0]] 58.898\n",
      " > 18.863\n",
      " > 18.606\n",
      " > 18.656\n",
      " > 18.722\n",
      " > 21.560\n",
      " > 19.503\n",
      " > 18.973\n",
      " > 19.832\n",
      " > 18.090\n",
      " > 19.991\n",
      " > Model[[12, 50, 100, 150, 12]] 19.280\n",
      " > 15.751\n",
      " > 18.820\n",
      " > 21.310\n",
      " > 15.962\n",
      " > 35.030\n",
      " > 29.719\n",
      " > 20.059\n",
      " > 27.383\n",
      " > 22.176\n",
      " > 21.940\n",
      " > Model[[12, 100, 100, 1, 0]] 22.815\n",
      " > 22.147\n",
      " > 20.189\n",
      " > 17.840\n",
      " > 19.356\n",
      " > 19.734\n",
      " > 21.663\n",
      " > 18.816\n",
      " > 19.022\n",
      " > 20.736\n",
      " > 17.598\n",
      " > Model[[12, 100, 100, 1, 12]] 19.710\n",
      " > 50.079\n",
      " > 34.635\n",
      " > 81.930\n",
      " > 40.456\n",
      " > 61.250\n",
      " > 54.465\n",
      " > 59.015\n",
      " > 68.742\n",
      " > 71.408\n",
      " > 71.167\n",
      " > Model[[12, 100, 100, 150, 0]] 59.315\n",
      " > 19.630\n",
      " > 19.084\n",
      " > 18.792\n",
      " > 20.358\n",
      " > 19.231\n",
      " > 19.390\n",
      " > 19.664\n",
      " > 17.908\n",
      " > 19.661\n",
      " > 19.441\n",
      " > Model[[12, 100, 100, 150, 12]] 19.316\n",
      "done\n",
      "[12, 50, 100, 150, 12] 19.279638912835292\n",
      "[12, 100, 100, 150, 12] 19.315871413295433\n",
      "[12, 100, 100, 1, 12] 19.709992281540455\n"
     ]
    }
   ],
   "source": [
    "# grid search mlps for monthly airline passengers dataset\n",
    "from math import sqrt\n",
    "from numpy import array\n",
    "from numpy import mean\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# split a univariate dataset into train/test sets\n",
    "def train_test_split(data, n_test):\n",
    "\treturn data[:-n_test], data[-n_test:]\n",
    "\n",
    "# transform list into supervised learning format \n",
    "def series_to_supervised(data, n_in, n_out=1):\n",
    "    # param: data, previous steps, output size\n",
    "    # return: shifted data with target in list\n",
    "    df = DataFrame(data)\n",
    "    cols = []\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        \n",
    "    # put it all together\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "    agg = concat(cols, axis=1)\n",
    "    # drop rows with NaN values \n",
    "    agg.dropna(inplace=True)\n",
    "    return agg.values \n",
    "\n",
    "# root mean squared error\n",
    "def measure_rmse(actual, predicted):\n",
    "\treturn sqrt(mean_squared_error(actual, predicted))\n",
    "\n",
    "# difference dataset\n",
    "def difference(data, order):\n",
    "    return [data[i] - data[i - order] for i in range(order, len(data))]\n",
    "\n",
    "# fit a model \n",
    "def model_fit(train, config):\n",
    "    # unpack config\n",
    "    n_input, n_nodes, n_epochs, n_batch, n_diff = config\n",
    "    # prepare data \n",
    "    if n_diff > 0:\n",
    "        train = difference(train, n_diff)\n",
    "    # transform series into supervised format \n",
    "    data = series_to_supervised(train, n_in=n_input)\n",
    "    # separate inputs and outputs \n",
    "    train_x, train_y = data[:, :-1], data[:, -1]\n",
    "    # define model \n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_nodes, activation='relu', input_dim=n_input))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    # fit model \n",
    "    model.fit(train_x, train_y, epochs=n_epochs, batch_size=n_batch, verbose=0)\n",
    "    return model \n",
    "\n",
    "# forecast with the fit model \n",
    "def model_predict(model, history, config):\n",
    "    # unpack config\n",
    "    n_input, _, _, _, n_diff = config \n",
    "    # prepare data \n",
    "    correction = 0.0 \n",
    "    # do the differencing for test data\n",
    "    if n_diff > 0:\n",
    "        correction = history[-n_diff]\n",
    "        history = difference(history, n_diff)\n",
    "    # shape input for model \n",
    "    x_input = array(history[-n_input:]).reshape((1, n_input))\n",
    "    # make forecast \n",
    "    yhat = model.predict(x_input, verbose=0)\n",
    "    # correct forecast if it was differenced\n",
    "    return correction + yhat[0]\n",
    "\n",
    "# walk-forward validation for univariate data \n",
    "def walk_forward_validation(data, n_test, cfg):\n",
    "    predictions = []\n",
    "    # split dataset \n",
    "    train, test = train_test_split(data, n_test)\n",
    "    # fit model \n",
    "    model = model_fit(train, cfg)\n",
    "    # seed history with training dataset \n",
    "    history = [x for x in train]\n",
    "    # step over each time-step in the test set \n",
    "    for i in range(len(test)):\n",
    "        # fit model and make forecast for history \n",
    "        yhat = model_predict(model, history, cfg)\n",
    "        # store forecast in list of predictions \n",
    "        predictions.append(yhat)\n",
    "        # add actual observation to history for the next loop \n",
    "        history.append(test[i])\n",
    "    # estimate prediction error \n",
    "    error = measure_rmse(test, predictions)\n",
    "    print(' > %.3f' % error)\n",
    "    return error \n",
    "\n",
    "# score a model, return None on failure \n",
    "def repeat_evaluate(data, config, n_test, n_repeats=10):\n",
    "    # convert config to a key \n",
    "    key = str(config)\n",
    "    # fit and evaluate the model n times \n",
    "    scores = [walk_forward_validation(data, n_test, config) for _ in range(n_repeats)]\n",
    "    # summarize score \n",
    "    result = mean(scores)\n",
    "    print(' > Model[%s] %.3f' % (key, result))\n",
    "    return (key, result)\n",
    "\n",
    "# grid search configs \n",
    "def grid_search(data, cfg_list, n_test):\n",
    "    # evaluate configs \n",
    "    scores = [repeat_evaluate(data, cfg, n_test) for cfg in cfg_list]\n",
    "    # sort configs by error, asc \n",
    "    scores.sort(key=lambda tup: tup[1])\n",
    "    return scores \n",
    "\n",
    "# create a list of configs to try \n",
    "def model_configs():\n",
    "\t# define scope of configs\n",
    "\tn_input = [12]\n",
    "\tn_nodes = [50, 100]\n",
    "\tn_epochs = [100]\n",
    "\tn_batch = [1, 150]\n",
    "\tn_diff = [0, 12]\n",
    "\t# create configs\n",
    "\tconfigs = []\n",
    "\tfor i in n_input:\n",
    "\t\tfor j in n_nodes:\n",
    "\t\t\tfor k in n_epochs:\n",
    "\t\t\t\tfor l in n_batch:\n",
    "\t\t\t\t\tfor m in n_diff:\n",
    "\t\t\t\t\t\tcfg = [i, j, k, l, m]\n",
    "\t\t\t\t\t\tconfigs.append(cfg)\n",
    "\tprint('Total configs: %d' % len(configs))\n",
    "\treturn configs\n",
    "\n",
    "# define dataset\n",
    "series = read_csv('monthly-airline-passengers.csv', header=0, index_col=0)\n",
    "data = series.values\n",
    "# data split\n",
    "n_test = 12\n",
    "# model configs\n",
    "cfg_list = model_configs()\n",
    "# grid search\n",
    "scores = grid_search(data, cfg_list, n_test)\n",
    "print('done')\n",
    "# list top 3 configs\n",
    "for cfg, error in scores[:3]:\n",
    "\tprint(cfg, error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A skillful model configuration was found as compared to a naive model that reported an RMSE of 50.70. We can see that the best RMSE of 19.27 was achieved with a configuration of [12, 100, 100, 1, 12], which we know can be interpreted as:\n",
    "- n_input: 12\n",
    "- n_nodes: 100 􏰀 \n",
    "- n_epochs: 100 􏰀 \n",
    "- n_batch: 1\n",
    "- n_diff: 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.5 Convolutional Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chosen set of hyperparameters to grid search in the CNN model are as follows:\n",
    "- n input: The number of prior inputs to use as input for the model (e.g. 12 months). 􏰀 \n",
    "- n filters: The number of filter maps in the convolutional layer (e.g. 32).\n",
    "- n kernel: The kernel size in the convolutional layer (e.g. 3).\n",
    "- n epochs: The number of training epochs (e.g. 1000).\n",
    "- n batch: The number of samples to include in each mini-batch (e.g. 32). 􏰀 \n",
    "- n diff: The difference order (e.g. 0 or 12)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0623 18:28:40.130493 4734326208 deprecation_wrapper.py:118] From /Users/Jianhua/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total configs: 8\n",
      " > 19.852\n",
      " > 16.230\n",
      " > 17.769\n",
      " > 22.177\n",
      " > 19.754\n",
      " > 19.298\n",
      " > 24.403\n",
      " > 17.817\n",
      " > 21.161\n",
      " > 17.449\n",
      "> Model[[12, 64, 3, 100, 1, 0]] 19.591\n",
      " > 21.661\n",
      " > 20.019\n",
      " > 20.847\n",
      " > 20.650\n",
      " > 20.818\n",
      " > 20.339\n",
      " > 19.604\n",
      " > 19.798\n",
      " > 21.242\n",
      " > 19.041\n",
      "> Model[[12, 64, 3, 100, 1, 12]] 20.402\n",
      " > 79.790\n",
      " > 67.060\n",
      " > 83.788\n",
      " > 75.714\n",
      " > 85.094\n",
      " > 85.625\n",
      " > 76.226\n",
      " > 78.647\n",
      " > 80.486\n",
      " > 72.742\n",
      "> Model[[12, 64, 3, 100, 150, 0]] 78.517\n",
      " > 19.912\n",
      " > 19.940\n",
      " > 19.154\n",
      " > 19.046\n",
      " > 19.559\n",
      " > 18.778\n",
      " > 18.389\n",
      " > 19.202\n",
      " > 18.675\n",
      " > 19.409\n",
      "> Model[[12, 64, 3, 100, 150, 12]] 19.207\n",
      " > 22.837\n",
      " > 27.479\n",
      " > 18.144\n",
      " > 22.803\n",
      " > 23.527\n",
      " > 22.823\n",
      " > 30.047\n",
      " > 22.347\n",
      " > 20.943\n",
      " > 25.418\n",
      "> Model[[12, 64, 5, 100, 1, 0]] 23.637\n",
      " > 19.103\n",
      " > 19.783\n",
      " > 18.275\n",
      " > 20.164\n",
      " > 17.683\n",
      " > 21.321\n",
      " > 19.076\n",
      " > 17.690\n",
      " > 18.976\n",
      " > 18.318\n",
      "> Model[[12, 64, 5, 100, 1, 12]] 19.039\n",
      " > 88.324\n",
      " > 96.536\n",
      " > 84.830\n",
      " > 88.879\n",
      " > 78.961\n",
      " > 87.690\n",
      " > 95.551\n",
      " > 80.023\n",
      " > 83.343\n",
      " > 88.648\n",
      "> Model[[12, 64, 5, 100, 150, 0]] 87.278\n",
      " > 19.096\n",
      " > 20.097\n",
      " > 20.360\n",
      " > 19.622\n",
      " > 18.594\n",
      " > 19.350\n",
      " > 21.494\n",
      " > 19.021\n",
      " > 19.799\n",
      " > 19.059\n",
      "> Model[[12, 64, 5, 100, 150, 12]] 19.649\n",
      "done\n",
      "[12, 64, 5, 100, 1, 12] 19.038966044816796\n",
      "[12, 64, 3, 100, 150, 12] 19.20652211046636\n",
      "[12, 64, 3, 100, 1, 0] 19.590934342151975\n"
     ]
    }
   ],
   "source": [
    "# grid search cnn for monthly airline passengers dataset\n",
    "from math import sqrt\n",
    "from numpy import array\n",
    "from numpy import mean\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "# split a univariate dataset into train/test sets\n",
    "def train_test_split(data, n_test):\n",
    "\treturn data[:-n_test], data[-n_test:]\n",
    "\n",
    "# transform list into supervised learning format\n",
    "def series_to_supervised(data, n_in, n_out=1):\n",
    "\tdf = DataFrame(data)\n",
    "\tcols = list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\t# drop rows with NaN values\n",
    "\tagg.dropna(inplace=True)\n",
    "\treturn agg.values\n",
    "\n",
    "# root mean squared error or rmse\n",
    "def measure_rmse(actual, predicted):\n",
    "\treturn sqrt(mean_squared_error(actual, predicted))\n",
    "\n",
    "# difference dataset\n",
    "def difference(data, order):\n",
    "\treturn [data[i] - data[i - order] for i in range(order, len(data))]\n",
    "\n",
    "# fit a model\n",
    "def model_fit(train, config):\n",
    "\t# unpack config\n",
    "\tn_input, n_filters, n_kernel, n_epochs, n_batch, n_diff = config\n",
    "\t# prepare data\n",
    "\tif n_diff > 0:\n",
    "\t\ttrain = difference(train, n_diff)\n",
    "\t# transform series into supervised format\n",
    "\tdata = series_to_supervised(train, n_in=n_input)\n",
    "\t# separate inputs and outputs\n",
    "\ttrain_x, train_y = data[:, :-1], data[:, -1]\n",
    "\t# reshape input data into [samples, timesteps, features]\n",
    "\tn_features = 1\n",
    "\ttrain_x = train_x.reshape((train_x.shape[0], train_x.shape[1], n_features))\n",
    "\t# define model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv1D(filters=n_filters, kernel_size=n_kernel, activation='relu', input_shape=(n_input, n_features)))\n",
    "\tmodel.add(MaxPooling1D(pool_size=2))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(1))\n",
    "\tmodel.compile(loss='mse', optimizer='adam')\n",
    "\t# fit\n",
    "\tmodel.fit(train_x, train_y, epochs=n_epochs, batch_size=n_batch, verbose=0)\n",
    "\treturn model\n",
    "\n",
    "# forecast with the fit model\n",
    "def model_predict(model, history, config):\n",
    "\t# unpack config\n",
    "\tn_input, _, _, _, _, n_diff = config\n",
    "\t# prepare data\n",
    "\tcorrection = 0.0\n",
    "\tif n_diff > 0:\n",
    "\t\tcorrection = history[-n_diff]\n",
    "\t\thistory = difference(history, n_diff)\n",
    "\tx_input = array(history[-n_input:]).reshape((1, n_input, 1))\n",
    "\t# forecast\n",
    "\tyhat = model.predict(x_input, verbose=0)\n",
    "\treturn correction + yhat[0]\n",
    "\n",
    "# walk-forward validation for univariate data\n",
    "def walk_forward_validation(data, n_test, cfg):\n",
    "\tpredictions = list()\n",
    "\t# split dataset\n",
    "\ttrain, test = train_test_split(data, n_test)\n",
    "\t# fit model\n",
    "\tmodel = model_fit(train, cfg)\n",
    "\t# seed history with training dataset\n",
    "\thistory = [x for x in train]\n",
    "\t# step over each time-step in the test set\n",
    "\tfor i in range(len(test)):\n",
    "\t\t# fit model and make forecast for history\n",
    "\t\tyhat = model_predict(model, history, cfg)\n",
    "\t\t# store forecast in list of predictions\n",
    "\t\tpredictions.append(yhat)\n",
    "\t\t# add actual observation to history for the next loop\n",
    "\t\thistory.append(test[i])\n",
    "\t# estimate prediction error\n",
    "\terror = measure_rmse(test, predictions)\n",
    "\tprint(' > %.3f' % error)\n",
    "\treturn error\n",
    "\n",
    "# score a model, return None on failure\n",
    "def repeat_evaluate(data, config, n_test, n_repeats=10):\n",
    "\t# convert config to a key\n",
    "\tkey = str(config)\n",
    "\t# fit and evaluate the model n times\n",
    "\tscores = [walk_forward_validation(data, n_test, config) for _ in range(n_repeats)]\n",
    "\t# summarize score\n",
    "\tresult = mean(scores)\n",
    "\tprint('> Model[%s] %.3f' % (key, result))\n",
    "\treturn (key, result)\n",
    "\n",
    "# grid search configs\n",
    "def grid_search(data, cfg_list, n_test):\n",
    "\t# evaluate configs\n",
    "\tscores = scores = [repeat_evaluate(data, cfg, n_test) for cfg in cfg_list]\n",
    "\t# sort configs by error, asc\n",
    "\tscores.sort(key=lambda tup: tup[1])\n",
    "\treturn scores\n",
    "\n",
    "# create a list of configs to try\n",
    "def model_configs():\n",
    "\t# define scope of configs\n",
    "\tn_input = [12]\n",
    "\tn_filters = [64]\n",
    "\tn_kernels = [3, 5]\n",
    "\tn_epochs = [100]\n",
    "\tn_batch = [1, 150]\n",
    "\tn_diff = [0, 12]\n",
    "\t# create configs\n",
    "\tconfigs = list()\n",
    "\tfor a in n_input:\n",
    "\t\tfor b in n_filters:\n",
    "\t\t\tfor c in n_kernels:\n",
    "\t\t\t\tfor d in n_epochs:\n",
    "\t\t\t\t\tfor e in n_batch:\n",
    "\t\t\t\t\t\tfor f in n_diff:\n",
    "\t\t\t\t\t\t\tcfg = [a,b,c,d,e,f]\n",
    "\t\t\t\t\t\t\tconfigs.append(cfg)\n",
    "\tprint('Total configs: %d' % len(configs))\n",
    "\treturn configs\n",
    "\n",
    "# define dataset\n",
    "series = read_csv('monthly-airline-passengers.csv', header=0, index_col=0)\n",
    "data = series.values\n",
    "# data split\n",
    "n_test = 12\n",
    "# model configs\n",
    "cfg_list = model_configs()\n",
    "# grid search\n",
    "scores = grid_search(data, cfg_list, n_test)\n",
    "print('done')\n",
    "# list top 10 configs\n",
    "for cfg, error in scores[:3]:\n",
    "\tprint(cfg, error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that a configuration of [12, 64, 5, 100, 1, 12] achieved an RMSE of 19.04, which is skillful as compared to a naive forecast model that achieved 50.70. We can unpack this configuration as:\n",
    "- n input: 12\n",
    "- n filters: 64 􏰀 \n",
    "- n kernel: 5\n",
    "- n epochs: 100 􏰀 \n",
    "- n batch: 1\n",
    "- n diff: 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.6 Long Short-Term Memory Network Model\n",
    "\n",
    "The hyperparameters for the LSTM model will be the same five as the MLP; they are:\n",
    "- n_input: The number of prior inputs to use as input for the model (e.g. 12 months). 􏰀 \n",
    "- n_nodes: The number of nodes to use in the hidden layer (e.g. 50).\n",
    "- n_epochs: The number of training epochs (e.g. 1000).\n",
    "- n_batch: The number of samples to include in each mini-batch (e.g. 32).\n",
    "- n_diff: The difference order (e.g. 0 or 12)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total configs: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0624 10:15:01.311615 4734326208 deprecation.py:323] From /Users/Jianhua/anaconda/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1251: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > 30.930\n",
      " > 22.318\n",
      " > 24.037\n",
      " > 20.242\n",
      " > 21.502\n",
      " > 23.923\n",
      " > 21.518\n",
      " > 19.279\n",
      " > 18.074\n",
      " > 19.779\n",
      "> Model[[12, 100, 50, 1, 12]] 22.160\n",
      " > 20.772\n",
      " > 19.448\n",
      " > 14.137\n",
      " > 21.546\n",
      " > 16.690\n",
      " > 23.050\n",
      " > 19.585\n",
      " > 22.027\n",
      " > 19.316\n",
      " > 23.455\n",
      "> Model[[12, 100, 50, 150, 12]] 20.003\n",
      "done\n",
      "[12, 100, 50, 150, 12] 20.002578701342053\n",
      "[12, 100, 50, 1, 12] 22.160071115291938\n"
     ]
    }
   ],
   "source": [
    "# grid search lstm for monthly airline passengers dataset\n",
    "from math import sqrt\n",
    "from numpy import array\n",
    "from numpy import mean\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "# split a univariate dataset into train/test sets\n",
    "def train_test_split(data, n_test):\n",
    "\treturn data[:-n_test], data[-n_test:]\n",
    "\n",
    "# transform list into supervised learning format\n",
    "def series_to_supervised(data, n_in, n_out=1):\n",
    "\tdf = DataFrame(data)\n",
    "\tcols = list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\t# drop rows with NaN values\n",
    "\tagg.dropna(inplace=True)\n",
    "\treturn agg.values\n",
    "\n",
    "# root mean squared error or rmse\n",
    "def measure_rmse(actual, predicted):\n",
    "\treturn sqrt(mean_squared_error(actual, predicted))\n",
    "\n",
    "# difference dataset\n",
    "def difference(data, order):\n",
    "\treturn [data[i] - data[i - order] for i in range(order, len(data))]\n",
    "\n",
    "# fit a model\n",
    "def model_fit(train, config):\n",
    "\t# unpack config\n",
    "\tn_input, n_nodes, n_epochs, n_batch, n_diff = config\n",
    "\t# prepare data\n",
    "\tif n_diff > 0:\n",
    "\t\ttrain = difference(train, n_diff)\n",
    "\t# transform series into supervised format\n",
    "\tdata = series_to_supervised(train, n_in=n_input)\n",
    "\t# separate inputs and outputs\n",
    "\ttrain_x, train_y = data[:, :-1], data[:, -1]\n",
    "\t# reshape input data into [samples, timesteps, features]\n",
    "\tn_features = 1\n",
    "\ttrain_x = train_x.reshape((train_x.shape[0], train_x.shape[1], n_features))\n",
    "\t# define model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(LSTM(n_nodes, activation='relu', input_shape=(n_input, n_features)))\n",
    "\tmodel.add(Dense(n_nodes, activation='relu'))\n",
    "\tmodel.add(Dense(1))\n",
    "\tmodel.compile(loss='mse', optimizer='adam')\n",
    "\t# fit model\n",
    "\tmodel.fit(train_x, train_y, epochs=n_epochs, batch_size=n_batch, verbose=0)\n",
    "\treturn model\n",
    "\n",
    "# forecast with the fit model\n",
    "def model_predict(model, history, config):\n",
    "\t# unpack config\n",
    "\tn_input, _, _, _, n_diff = config\n",
    "\t# prepare data\n",
    "\tcorrection = 0.0\n",
    "\tif n_diff > 0:\n",
    "\t\tcorrection = history[-n_diff]\n",
    "\t\thistory = difference(history, n_diff)\n",
    "\t# reshape sample into [samples, timesteps, features]\n",
    "\tx_input = array(history[-n_input:]).reshape((1, n_input, 1))\n",
    "\t# forecast\n",
    "\tyhat = model.predict(x_input, verbose=0)\n",
    "\treturn correction + yhat[0]\n",
    "\n",
    "# walk-forward validation for univariate data\n",
    "def walk_forward_validation(data, n_test, cfg):\n",
    "\tpredictions = list()\n",
    "\t# split dataset\n",
    "\ttrain, test = train_test_split(data, n_test)\n",
    "\t# fit model\n",
    "\tmodel = model_fit(train, cfg)\n",
    "\t# seed history with training dataset\n",
    "\thistory = [x for x in train]\n",
    "\t# step over each time-step in the test set\n",
    "\tfor i in range(len(test)):\n",
    "\t\t# fit model and make forecast for history\n",
    "\t\tyhat = model_predict(model, history, cfg)\n",
    "\t\t# store forecast in list of predictions\n",
    "\t\tpredictions.append(yhat)\n",
    "\t\t# add actual observation to history for the next loop\n",
    "\t\thistory.append(test[i])\n",
    "\t# estimate prediction error\n",
    "\terror = measure_rmse(test, predictions)\n",
    "\tprint(' > %.3f' % error)\n",
    "\treturn error\n",
    "\n",
    "# score a model, return None on failure\n",
    "def repeat_evaluate(data, config, n_test, n_repeats=10):\n",
    "\t# convert config to a key\n",
    "\tkey = str(config)\n",
    "\t# fit and evaluate the model n times\n",
    "\tscores = [walk_forward_validation(data, n_test, config) for _ in range(n_repeats)]\n",
    "\t# summarize score\n",
    "\tresult = mean(scores)\n",
    "\tprint('> Model[%s] %.3f' % (key, result))\n",
    "\treturn (key, result)\n",
    "\n",
    "# grid search configs\n",
    "def grid_search(data, cfg_list, n_test):\n",
    "\t# evaluate configs\n",
    "\tscores = scores = [repeat_evaluate(data, cfg, n_test) for cfg in cfg_list]\n",
    "\t# sort configs by error, asc\n",
    "\tscores.sort(key=lambda tup: tup[1])\n",
    "\treturn scores\n",
    "\n",
    "# create a list of configs to try\n",
    "def model_configs():\n",
    "\t# define scope of configs\n",
    "\tn_input = [12]\n",
    "\tn_nodes = [100]\n",
    "\tn_epochs = [50]\n",
    "\tn_batch = [1, 150]\n",
    "\tn_diff = [12]\n",
    "\t# create configs\n",
    "\tconfigs = list()\n",
    "\tfor i in n_input:\n",
    "\t\tfor j in n_nodes:\n",
    "\t\t\tfor k in n_epochs:\n",
    "\t\t\t\tfor l in n_batch:\n",
    "\t\t\t\t\tfor m in n_diff:\n",
    "\t\t\t\t\t\tcfg = [i, j, k, l, m]\n",
    "\t\t\t\t\t\tconfigs.append(cfg)\n",
    "\tprint('Total configs: %d' % len(configs))\n",
    "\treturn configs\n",
    "\n",
    "# define dataset\n",
    "series = read_csv('monthly-airline-passengers.csv', header=0, index_col=0)\n",
    "data = series.values\n",
    "# data split\n",
    "n_test = 12\n",
    "# model configs\n",
    "cfg_list = model_configs()\n",
    "# grid search\n",
    "scores = grid_search(data, cfg_list, n_test)\n",
    "print('done')\n",
    "# list top 10 configs\n",
    "for cfg, error in scores[:3]:\n",
    "\tprint(cfg, error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example, we can see that only two distinct configurations are evaluated. We can see that a configuration of [12, 100, 50, 1, 12] achieved an RMSE of 20.00, which is skillful as compared to a naive forecast model that achieved 50.70. The model requires a lot more tuning and may do much better with a hybrid configuration, such as having a CNN model as input. We can unpack this configuration as:\n",
    "- n input: 12 􏰀 \n",
    "- n nodes: 100 􏰀 \n",
    "- n epochs: 50 􏰀 \n",
    "- n batch: 1\n",
    "- n diff: 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.8 Further Reading\n",
    "This section provides more resources on the topic if you are looking to go deeper.\n",
    "- Keras Sequential Model API. https://keras.io/models/sequential/\n",
    "- Keras Core Layers API. https://keras.io/layers/core/\n",
    "- Keras Convolutional Layers API. https://keras.io/layers/convolutional/\n",
    "- Keras Pooling Layers API. https://keras.io/layers/pooling/\n",
    "- Keras Recurrent Layers API. https://keras.io/layers/recurrent/\n",
    "\n",
    "## 15.9 Summary\n",
    "In this tutorial, you discovered how to develop a framework to grid search hyperparameters for deep learning models. Specifically, you learned:\n",
    "- How to develop a generic grid searching framework for tuning model hyperparameters.\n",
    "- How to grid search hyperparameters for a Multilayer Perceptron model on the airline passengers univariate time series forecasting problem.\n",
    "- How to adapt the framework to grid search hyperparameters for convolutional and long short-term memory neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
